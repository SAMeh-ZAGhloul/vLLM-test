{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNPf/hVkorvWmnGqSiw7hjq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SAMeh-ZAGhloul/vLLM-test/blob/main/vLLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://medium.com/@fengliplatform/trying-out-vllm-in-colab-459484096386"
      ],
      "metadata": {
        "id": "uQP7PJMHRxzY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "k1nSk0f5RLPg"
      },
      "outputs": [],
      "source": [
        "!uv pip install -q vllm kaleido python-multipart typing-extensions==4.5.0 torch==2.1.0 \"numpy<2\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "# Download Facebook OPT 125M model and load to local variable llm\n",
        "llm = LLM(model=\"facebook/opt-125m\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz5W26JlRN_r",
        "outputId": "2886bf67-bbf9-498c-8d2a-c71a8e7219de"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 07-20 10:41:38 llm_engine.py:73] Initializing an LLM engine with config: model='facebook/opt-125m', tokenizer='facebook/opt-125m', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, quantization=None, seed=0)\n",
            "MegaBlocks not found. Please install it by `pip install megablocks`. Note that MegaBlocks depends on mosaicml-turbo, which only supports Python 3.10 for now.\n",
            "STK not found: please see https://github.com/stanford-futuredata/stk\n",
            "INFO 07-20 10:41:41 llm_engine.py:222] # GPU blocks: 22431, # CPU blocks: 7281\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"Hello, my name is\",\n",
        "    \"The president of the United States is\",\n",
        "    \"The capital of France is\",\n",
        "    \"The future of AI is\",\n",
        "]\n",
        "sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n",
        "\n",
        "outputs = llm.generate(prompts, sampling_params)\n",
        "\n",
        "# Print the outputs.\n",
        "for output in outputs:\n",
        "    prompt = output.prompt\n",
        "    generated_text = output.outputs[0].text\n",
        "    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqzbcETfRN9E",
        "outputId": "c6f481a4-78a2-4887-a602-f15ae35672a6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 4/4 [00:00<00:00, 23.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: 'Hello, my name is', Generated text: \" Joel.  I'm a homemaker.  I drink tea a lot and\"\n",
            "Prompt: 'The president of the United States is', Generated text: ' giving his blessing to new \"master\" food manufacturing technology that could revolutionize food'\n",
            "Prompt: 'The capital of France is', Generated text: ' now the capital of the French Resistance\\nThis is a really bad troll.'\n",
            "Prompt: 'The future of AI is', Generated text: ' now at a crossroads\\nFor now, the gap between AI-based solutions'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GfHd_AGwRN6f"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}